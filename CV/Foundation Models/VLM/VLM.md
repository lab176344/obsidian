
VIsion Language Models are large LLMs which mixes languages and vision and decodes the output



| Paper                                                                                | Summary                                                                                                                                                                                                                | Github |
| ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ |
| [[What matters when building a VLM]]                                                 | Paper discussing the important considerations when building VLMs<br>                                                                                                                                                   |        |
| [[Alpha-CLIP - A CLIP Model Focusing on Wherever You Want]]                          | CLIP with possibilities to add auxiliary information about the objects                                                                                                                                                 |        |
| [[Florence 2]]                                                                       | Ultra small VLM that are pretrained with a diverse set of tasks                                                                                                                                                        |        |
| Phi35                                                                                |                                                                                                                                                                                                                        |        |
| Qwen                                                                                 |                                                                                                                                                                                                                        |        |
| Aria                                                                                 |                                                                                                                                                                                                                        |        |
| Allen Lab VLM                                                                        |                                                                                                                                                                                                                        |        |
| <br>[Chain of Though Reasoning without prompting]()                                  | Uses smart sampling strategies to enforce model to invoke CoT                                                                                                                                                          |        |
| LLama Vision                                                                         |                                                                                                                                                                                                                        |        |
| [[LingoQA Video Question Answering for Autonomous Driving]]                          | A novel dataset and an interested way to evaluate VLMs by training a judge model and treating it as a classification problem                                                                                           |        |
| [[One Token to Seg Them All -  Language InstructedReasoning Segmentation in Videos]] | The aim is to develop a model to sugment objects in video and track them based language queries based on [[Segment Anything 2 + Grounding Dino]]                                                                       |        |
| [[Segment Anything 2 + Grounding Dino]]                                              |                                                                                                                                                                                                                        |        |
| [[LLAVA - Critic]]                                                                   | **large multimodal model (LMM) designed as a generalist evaluator**Â to assess performance across a wide range of multimodal tasks similar to LingoQA judge [[LingoQA Video Question Answering for Autonomous Driving]] |        |
| [[LLAVA - Video - VIDEO INSTRUCTION TUNING WITH SYNTHETIC DATA]]                     | Another distillation dataset paper, pyscene based video selection is interesting                                                                                                                                       |        |
| [[VLM2VEC -  TRAINING VISION-LANGUAGE MODELSFOR MASSIVE MULTIMODAL EMBEDDING TASKS]] | Fine tuning VLM to do video embedding tasks                                                                                                                                                                            |        |















