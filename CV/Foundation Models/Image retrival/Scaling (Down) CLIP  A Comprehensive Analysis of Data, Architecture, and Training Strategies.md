The paper mainly deals with understanding how to train CLIP style models including the anylsis of data efficiency, data quality, architecture choices etc.


The main conclusions are as follows:
1. We train on datasets of different sizes to explore if the performance on ImageNet variants align with the performance on ImageNet. We also explore the importance of data quality and demonstrate that a smaller set of high-quality data can outperform larger datasets. We find that using only the higher quality 40% of the data can achieve better performance than using the entire dataset, and we investigate how model performance changes with increasing dataset sizes. Our results provide guidance on how to select training data for CLIP models, a critical issue for practical applications.

2. Concerning architecture, we compare multiple architectures under various dataset sizes. We investigate how to choose size of the vision encoder depending on the dataset size and compute budget. We show that the a larger ViT (Dosovitskiy et al., 2020) is not always better. We also demonstrate the importance of selecting between CNN (He et al., 2016) and ViT architectures for CLIP training. Although previous studies have shown that ViT-based CLIP has better performance than CNN-based CLIP models, we find that when the number of sampled data is small, CNNs perform better.

3. Finally, we compare four options: SLIP (Mu et al., 2021), FLIP (Li et al., 2022), CLIP, and CLIP+Data Augmentation. We show that SLIP Mu et al. (2021) is not always the best choice compared to CLIP and FLIP. When the size of the training set is small, SLIP performs better than CLIP. However, as the training data size increases, SLIP has similar performance to CLIP but incurs twice the computational cost. Our results provide insights into the trade-offs between computational cost and performance in CLIP training, a critical issue for practical applications.

4.  Finally, in terms of training strategies, we compare four options: SLIP Mu et al. (2021), FLIP Li et al. (2022), CLIP, and CLIP+Data Augmentation. We show that SLIP Mu et al. (2021) does not always outperform CLIP and FLIP. When the size of the training set is small, SLIP performs better than CLIP. However, at larger training set sizes, SLIP has a similar performance to CLIP, but requires twice the computational cost. We explore the trade-offs between computational cost and performance.