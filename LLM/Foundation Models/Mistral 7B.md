
- Mixture of experts model

- 8 experts with a gating mechanism

* Each expert is good at one thing

*  Also uses sliding window attention for faster throughput 

* Uses a switch transformer

* Has an instruct version of the model

* Sparse Top K using soft max top k gating function

![[Pasted image 20240111095514.png]]



LLMs

Mistral 7B